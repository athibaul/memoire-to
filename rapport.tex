\input{../utils.tex}
\usepackage{subcaption}

\DeclareMathOperator{\diag}{\mathbf{diag}}
\DeclareMathOperator{\Ccal}{\mathcal{C}}

\newcommand{\tab}{\ \ \ \ }

\title{Transport optimal et régularisation}
\author{Alexis THIBAULT}
\date{2017}

\begin{document}

\maketitle

\section*{Sommaire}
La distance de transport optimal est une distance entre histogrammes, qui a de nombreuses applications en machine learning ou en traitement d'images, entre autres. La principale difficulté concrète est que ce problème de programmation linéaire devient très coûteux à résoudre pour plus de quelques centaines de points. Nous présentons ici le problème de transport optimal régularisé, approximant le problème de transport optimal en lissant les solutions. Nous présentons également un algorithme permettant de calculer efficacement la solution du problème régularisé par des opérations matricielles. Un exemple présente l'utilisation de cet algorithme pour l'étude de textes, et montre qu'il parvient à regrouper les ouvrages d'un même auteur à partir du champ lexical utilisé.


%\tableofcontents

\section{Introduction}
Dans de nombreux problèmes en médecine, en économie ou en machine learning, il est question de comparer des formes, des histogrammes, des mesures de probabilité.
Les distances classiques, telles que la norme $l^1$, ne suffisent pas toujours, en particulier dans le cas où la métrique de l'espace sous-jacent est importante. Alors la théorie du transport  optimal \cite{villani08} devient intéressante.

L'idée du transport optimal est de mesurer le coût minimal permettant de déplacer de la matière de façon à transformer une répartition de masse en une autre. Ce coût minimal est (sous certaines conditions) une distance sur l'espace des mesures de probabilité\cite{villani08}, qui admet des propriétés agréables.

En pratique, on peut vouloir mesurer à quel point deux formes géométriques sont différentes (par exemple des c\oe urs, en imagerie médicale). Le coût de transport optimal sera alors plus approprié que les normes basées sur des considérations ponctuelles. En effet, contrairement à ces normes, il fera plus grand cas de l'apparition ou la disparition d'une partie de la forme (par exemple un ventricule atrophié) que de la simple translation de cette partie. C'est donc une distance qui est également bien adaptée à la vision artificielle.

Malheureusement son calcul numérique est cher~: la complexité de calcul pour des histogrammes de taille $N$ est de l'ordre de $O(N^3 \log N)$ \cite{pele09}. C'est pourquoi nous chercherons à trouver des solutions approximatives plus efficacement.


\section{Transport optimal}

\subsection{Problème de Monge-Kantorovich}
La formulation initiale du problème de transport est due à Monge \cite{villani08}. Il est question de transporter une mesure de probabilité $\mu$ sur $\IR^d$ vers une autre $\nu$ par une application $T\,:\,\IR^d \rightarrow \IR^d$, de façon à ce que $\nu$ soit la mesure image de $\mu$ par $T$, en minimisant la quantité~:
\begin{equation}
\label{eq:monge}
M(T) = \int_{\IR^d} |T(x)-x|\,d\mu(x) .
\end{equation}
Il est possible de généraliser le problème en remplaçant $|T(x)-x|$ par un coût quelconque $c(T(x),x)$, ce qui permet de considérer que le problème n'est pas limité à $\IR^d$. On peut aussi décider que les espaces de départ et d'arrivée peuvent être différents.

Cette première formulation a des défauts~: il n'existe pas toujours une application $T$ envoyant $\mu$ sur $\nu$. Un contre-exemple direct est celui où $\mu$ est une distribution de Dirac en 0 et où $\nu$ est une distribution non localisée en un point, l'image de $\mu$ par $T$ étant alors toujours $\delta(x-T(0))$. Ainsi, le problème de Monge sera rarement utile dans les cas discrets~: il se ramène au problème de couplage optimal entre les points qui ont le même poids. Il se trouve qu'il est par ailleurs difficile à étudier mathématiquement.

C'est pourquoi Kantorovich a introduit \cite{kantorovich42} une autre formulation du problème, qui consiste à relâcher les contraintes sur la façon dont le transport s'effectue. Si $X$ et $Y$ sont respectivement l'espace de départ et l'espace d'arrivée du transport, Kantorovich considère les \emph{plans de transport}~:
\[\Pi(\mu,\nu) = \{\gamma\;\text{mesure de probabilité sur}\,X\times Y\;|\;(\pi_X)_\# \gamma = \mu,\; (\pi_Y)_\# \gamma = \nu\} , \]
où $(\pi_X)_\# \gamma$ dénote la mesure marginale de $\gamma$, c'est-à-dire la mesure image par projection sur $X$. \' Etant donnée une fonction de coût $c\, : \, X\times Y \rightarrow \IR_+\cup\,\{+\infty\}$ , la quantité que Kantorovich veut minimiser est alors~:
\begin{equation}
\label{eq:kantorovich}
K(\gamma) = \int_{X\times Y} c\,d\gamma \; ,\quad \text{où} \quad \gamma \in \Pi(\mu,\nu) .
\end{equation}
Il s'agit donc de minimiser une fonctionnelle linéaire sur le convexe $\Pi(\mu,\nu)$, ce qui en fait un problème de programmation linéaire.
\begin{theoreme}[Existence d'un plan de transport optimal]
On suppose que $X$ et $Y$ sont deux ensembles finis, chacun muni d'une loi de probabilité (respectivement $\mu$ et $\nu$). Alors il existe un plan de transport $\gamma \in \Pi(\mu,\nu)$ qui minimise le coût de transport $K(\gamma)$.
\end{theoreme}
La preuve d'un résultat plus général se trouve dans \cite{villani08}. Puisqu'on s'intéresse ici au cas discret pour les applications numériques, le résultat nous suffira dans le cadre beaucoup plus simple des ensembles finis.
\begin{proof}
On identifie $X$ à $\{1,\ldots,n\}$ et $Y$ à $\{1,\ldots,m\}$. Alors $\Pi(\mu,\nu)$ est immédiatement assimilable à une partie de $\IR^{\{1,\ldots,n\}\times\{1,\ldots,m\}}$, espace de dimension finie. $\Pi(\mu,\nu)$ est non vide car il contient le couplage indépendant $\gamma_{i,j} = \mu_i \nu_j$. Il est fermé car défini par un nombre fini d'égalités ($\sum_i \gamma_{i,j} = \nu_j$ ; $\sum_j \gamma_{i,j} = \mu_i$) et d'inégalités larges ($\gamma_{i,j} \ge 0$), qui passent donc à la limite. Il est borné car pour $\gamma \in \Pi(\mu,\nu)$, $0 \le \gamma_{i,j} \le 1$ pour tout $(i,j)$. Ainsi $\Pi(\mu,\nu)$ est un compact de $\IR^{\{1,\ldots,n\}\times\{1,\ldots,m\}}$. Enfin, $\gamma \mapsto \int c\,d\gamma = \sum_{i,j} c_{i,j} \gamma_{i,j}$ est continue sur $\IR^{\{1,\ldots,n\}\times\{1,\ldots,m\}}$, donc admet un minimum sur le compact non vide $\Pi(\mu,\nu)$.
\end{proof}

\subsection{Métrique de Wasserstein}
On s'intéresse maintenant au coût minimal nécessaire au transport~:
\[C(\mu,\nu) = \min_{\gamma \in \Pi(\mu,\nu)} K(\gamma). \]
On peut considérer ce coût comme une sorte de distance entre $\mu$ et $\nu$. Quelques hypothèses supplémentaires sont cependant nécessaires pour pouvoir affirmer que ceci constitue bien une distance au sens mathématique. Il suffit par exemple que $X=Y$ et que le coût soit défini à partir d'une distance \cite{villani08}.

\begin{definition}[Métrique de Wasserstein]
Soit $(X,d)$ espace métrique, et $p \in \IR,\;p \ge 1$. On définit la \emph{distance de Wasserstein} d'ordre $p$ entre $\mu$ et $\nu$ comme le coût de transport optimal entre ces mesures, avec $c = d^p$~:
\begin{equation}
\label{eq:wasserstein}
W_p(\mu,\nu) = \left(\inf_{\gamma \in \Pi(\mu,\nu)} \int_{X\times X} d(x,y)^p\,d\gamma(x,y) \right)^{\frac{1}{p}}.\end{equation}
\end{definition}

\begin{proof}[Preuve que $W_p$ vérifie les axiomes d'une distance]
La positivité et la symétrie sont claires.

Supposons $W_p(\mu,\nu) = 0$. Alors le plan de transport minimisant $K$ a son support inclus dans la diagonale $\{x=y\}$, et $\nu$ est la mesure image de $\mu$ par l'application identité~; autrement dit $\nu = \mu$.

Prouvons l'inégalité triangulaire. Soient $\mu^1$, $\mu^2$ et $\mu^3$ trois mesures de probabilité sur $X$. Soit $\gamma^{12}$ plan de transport optimal pour $(\mu^1,\mu^2)$ et $\gamma^{23}$ plan de transport optimal pour $(\mu^2,\mu^3)$. On construit alors une loi jointe sur $X^3$ par la formule~:
\[\gamma^{123}(A) = \frac{\gamma^{12}(A_{12}) \gamma^{23}(A_{23})}{\mu^2(A_2)} , \]
où $A_{12}$ est la projection sur les coordonnées 1 et 2 de l'ensemble $A$, et de même pour $A_{13}$ et $A_{2}$~;
de sorte que les marginales de $\gamma^{123}$ soient $\mu^1$, $\mu^2$ et $\mu^3$. On note \[\gamma^{13}(B) = \sum_{j} \gamma^{123}(\{(x_1,x_2,x_3) \in X^3 \,|\, (x_1,x_3) \in B\}).\] Alors~:
\[\begin{split}
W_p(\mu^1,\mu^3) & \le \left( \int_{x_1,x_3} d(x_1,x_3)^p \;d\gamma^{13}(x_1,x_3) \right)^{1/p} \\
&\le \left( \int_{x_1,x_2,x_3} (d(x_1,x_2) + d(x_2,x_3))^p \;d\gamma^{123}(x_1,x_2,x_3) \right)^{1/p} \\
& \le \left( \int_{x_1,x_2} d(x_1,x_2)^p \;d\gamma^{12}(x_1,x_2) \right)^{1/p} + \left( \int_{x_2,x_3} d(x_2,x_3)^p \;d\gamma^{23}(x_2,x_3) \right)^{1/p} \\
& \le W_p(\mu^1,\mu^2) + W_p(\mu^2,\mu^3).
\end{split}\]
où la troisième inégalité est une application de l'inégalité de Minkowski.
\end{proof}

Lorsque l'on travaille avec des espaces non compacts, il faut faire attention à ce que les intégrales définissant $W_p$ convergent effectivement. On se restreint donc aux mesures telles qui admettent un \og moment d'ordre $p$\fg, au sens suivant~:
\[P_p(X) = \left\{\mu \in P(X)\,\middle|\, \int d(x_0,x)^p d\mu(x) < +\infty \right\} , \]
où $x_0 \in X$ est quelconque. Cet espace ne dépend pas de $x_0$.

La topologie induite par $W_p$ sur l'espace $P_p(X)$ vérifie alors des propriétés assez satisfaisantes (prouvées dans \cite{villani08}, page 104)~: 
\begin{theoreme}[Topologie des espaces de Wasserstein]
Si $X$ est complet et séparable, alors $P_p(X) = (P(X),W_p)$ est un espace métrique complet et séparable. De plus, toute mesure de probabilité est limite d'une suite de mesures à support fini.
\end{theoreme}
\begin{remarque}
L'injection de $X$ dans $P_p(X)$, qui à chaque point associe la mesure de Dirac unitaire en ce point est une isométrie. En particulier, si $(x_n)$ converge vers $y$, alors la distance de Wasserstein entre les mesures de Dirac associées tend vers 0~:
\[\lim_{n \rightarrow \infty} W_p(\delta_{x_n},\delta_y) = 0. \]
\end{remarque}

On visualise assez bien la preuve de la deuxième partie du théorème dans le cas particulier où $X = [0;1]^d$.
\begin{proof}[Démonstration dans le cas où X est un cube]
On suppose $X = [0;1]^d$.
Soit $\mu \in P([0;1]^d)$. Pour $n \in \IN$, on peut diviser le cube en $2^{nd}$ cubes $(C_{i_1,\ldots,i_d})_{0 \le i_1,\ldots,i_d < 2^n}$ de côté $2^{-n}$, et considérer la mesure constituée des pics de Dirac normalisés, pris au coin de chaque cube~:
\[\nu_n = \sum_{0 \le i_1,\ldots,i_d < 2^n} \mu(C_{i_1,\ldots,i_d}) \delta_{a_{i_1,\ldots,i_d}} ,
\quad \text{où} \quad
a_{i_1,\ldots,i_d} = 2^{-n}(i_1,\ldots,i_d)^\transpose .\]
Un plan de transport possible de $\mu$ à $\nu_n$ est alors celui qui distribue chaque masse de Dirac sur le cube associé. La distance à parcourir n'étant pas trop grande, le coût du transport est majoré par une quantité qui tend vers 0.
\[
\begin{split}
\forall x \in C_{i_1,\ldots,i_d},\quad d(x,a_{i_1,\ldots,i_d}) & \le 2^{-n} \sqrt{d} \\
W_p(\mu,\nu_n) &\le \left(\int_{X\times X} d(a,x)^p\,d\gamma(a,x)\right)^{1/p} \\
&\le \left(\sum_{0 \le i_1,\ldots,i_d < 2^n} \int_{C_{i_1,\ldots,i_d}} d(x,a_{i_1,\ldots,i_d})^p \, d\mu (x) \right)^{1/p} \\
& \le 2^{-n}\sqrt{d} .
\end{split}
\]
Ainsi, $\nu_n \rightarrow_{n \rightarrow +\infty} \mu$.
\end{proof}

\subsection{Calcul numérique}
Le problème de Kantorovich est un problème d'optimisation linéaire.
Le calcul du transport optimal entre histogrammes de taille $n$ dans un espace métrique général prend une complexité $O(d^3 \log(d))$ par la méthode des points intérieurs \cite{pele09}.
Ce type d'algorithme est relativement compliqué, ne peut pas bénéficier de l'accélération physique d'un GPU, et il peut sembler plus intéressant d'avoir des résultats approchés pour un coût de calcul bien inférieur~: c'est pourquoi nous nous intéresserons à partir de maintenant à la question de l'\emph{approximation du transport optimal}.

\section{Problème régularisé}
Dans le but de calculer le transport optimal de façon plus efficace, une idée présentée dans \cite{cuturi13} consiste à se limiter aux solutions \og assez régulières \fg, en ajoutant un terme entropique dans l'expression du coût à minimiser. Cette méthode est en fait assez naturelle~: elle a simplement tendance à légèrement lisser les solutions. Le problème devient alors strictement convexe, ce qui permet d'assurer l'unicité de la solution, et le fait d'utiliser l'entropie assure que le plan de transport $\gamma$ a tous ses coefficients non nuls, ce qui simplifie le travail. En effet, la forme particulière du coût à régularisation entropique fait qu'il existe alors des algorithmes à convergence rapide vers la solution consistant en de simples produits matriciels --- et qui peuvent donc être implémentés efficacement sur GPU.

\subsection{Formulation}
On se limite dorénavant aux cas discrets. Le plan de transport s'écrit 
\begin{equation}\label{eq:pi_discret}
\Pi(\mu,\nu) = \left\{ \gamma \in \IR_+^{X \times Y} \quad\middle|\quad
\forall i \in X,\,\sum_{j \in Y} \gamma_{i,j} = \mu_i , \quad
 \text{et} \quad
\forall j \in Y ,\,\sum_{i \in X} \gamma_{i,j} = \nu_j \right\}.
\end{equation}
Le coût de transport selon la formulation de Kantorovich (non régularisé), était~:
\[K(\gamma) = \sum_{i,j} c_{i,j} \gamma_{i,j}.\]
On définit maintenant le coût régularisé comme~:
\begin{equation}\label{eq:keps}
\begin{split}
K^\epsilon (\gamma) 
 &= K(\gamma) + \epsilon S(\gamma) \\
&= \sum_{i,j} c_{i,j} \gamma_{i,j} + \epsilon \sum_{i,j} \gamma_{i,j} \log(\gamma_{i,j}) . 
\end{split}\end{equation}
On appelle $-S(\gamma)$ l'\emph{entropie} de $\gamma$, que l'on prolonge par continuité si certains coefficients de $\gamma$ sont nuls.

Quitte à diminuer le nombre de points de $X$ ou de $Y$, on peut supposer que tous les coefficients $\mu_i$ et $\nu_j$ sont non nuls, auquel cas on a le lemme suivant.
\begin{lemma}
Le coût $K^\epsilon$ est strictement convexe. Son unique minimum $\gamma^\epsilon$ a tous ses coefficients strictement positifs.
\end{lemma}
\begin{proof}
\begin{description}
\item[Stricte convexité]
$K$ étant linéaire, il suffit de montrer que $S$ est strictement convexe sur $\Pi(\mu,\nu)$.
La fonction $\phi\,:\, x \in \IR_+ \mapsto x \log x$, où par convention $0 \log 0 = 0$, est strictement convexe, car $\mathcal{C}^2$, de dérivée seconde strictement positive, et prolongée par continuité en 0.
Alors pour $(\lambda_k)$ famille finie de réels positifs sommant à 1, on a~:
\[\begin{split}
S(\gamma) &= \sum_{i,j} \phi(\gamma_{i,j}) \\
S\left(\sum_k \lambda_k \gamma^k\right) &= \sum_{i,j} \phi\left(\sum_k \lambda_k \gamma^k_{i,j}\right) \\
& \le \sum_{i,j} \sum_{k} \lambda_k \phi\left(\gamma^k_{i,j}\right) \\
& \le \sum_{k} \lambda_k S\left(\gamma^k\right) .
\end{split}\]
Pour avoir égalité, il faut avoir égalité dans toutes les inégalités de convexité appliqués à la troisième ligne. Il faut donc que pour tout $i,j$, si $\lambda_k$ et $\lambda_{k'}$ sont non nuls, on ait $\gamma_{i,j}^k = \gamma_{i,j}^{k'}$. Mais alors cela implique que $\gamma^k = \gamma^{k'}$, autrement dit le cas est dégénéré. $S$ est donc bien strictement convexe.
\item[Coefficients non nuls]
Montrons que le minimum $\gamma^\epsilon$ n'admet pas de coefficients nuls. Prenons $\gamma$ et $i_0,j_0$ tels que $\gamma_{i_0,j_0} = 0$. Posons $\gamma^u$ le transport associé au couplage indépendant~: $\gamma^u_{i,j} = \mu_i \nu_j$, puis pour $t \in [0;1]$,
\[ \eta^t = (1-t) \gamma + t \gamma^u \quad \in \Pi(\mu,\nu) .\]
On remarque que $\gamma^u$ a tous ses coefficients non nuls. On peut considérer la variation du coût en fonction de $t$~:
\[\begin{split}
\frac{d}{dt} K^\epsilon(\eta^t) &= \frac{d}{dt}\left( K(\eta^t) + \epsilon S(\eta^t) \right) \\
& = - K(\gamma) + K(\gamma^u) + \epsilon \sum_{i,j} \frac{d}{dt} \phi((1-t) \gamma_{i,j} + t \gamma^u_{i,j}) \\
& = f(t) + \epsilon \sum_{\substack{i,j \\ \gamma_{i,j} = 0}} \gamma^u_{i,j}\frac{d\phi}{dt}(t \gamma^u_{i,j})  ,
\end{split}\]
où $f$ est bornée au voisinage de 0. Ainsi, pour $t$ assez petit, les termes de droite tendent vers $-\infty$, et la dérivée est négative. On en conclut que $K^\epsilon(\eta^t) < K^\epsilon(\gamma)$ pour $t>0$ assez petit, donc que $\gamma$ n'était pas un minimum.
\end{description}
\end{proof}

\subsection{Convergence}
Nous avons modifié le coût d'une quantité qui tend vers 0 ; mais cela ne signifie a priori pas que les minima $\gamma^\epsilon$ aient un lien avec le minimum $\gamma$ de $K$.
\begin{lemma}
La fonction $\epsilon \mapsto \gamma^\epsilon$ admet une limite $\gamma^0$ en 0.
Ce $\gamma^0$ est un minimum de $K$, qui minimise l'entropie sur l'ensemble des minima de $K$.
\end{lemma}
\begin{proof}
Remarquons que $\argmin_{\Pi(\mu,\nu)} K$ est convexe par linéarité de $K$, fermé dans $\Pi(\mu,\nu)$ car c'est l'image réciproque d'un singleton, puis compact et non vide par compacité de $\Pi(\mu,\nu)$.
Notons $\bar{\gamma}$ l'unique minimiseur de l'entropie $S$ sur $\argmin_{\Pi(\mu,\nu)} K$, et le coût correspondant $K_{opt} := K(\bar{\gamma})$.

On peut pour toute suite de $\epsilon$ tendant vers 0, extraire une sous-suite telle que $\gamma^\epsilon$ admette une limite $\gamma^0$. Montrons que cette limite est toujours $\bar{\gamma}$.

Le minimiseur $\gamma^\epsilon$ de $K^\epsilon$ est également minimiseur de 
\[C_\epsilon(\gamma) := \frac{K(\gamma) - K_{opt}}{\epsilon} + S(\gamma) .\]
Du fait que $\bar{\gamma}$ minimise $K$, on a $K - K_{opt} \ge 0$. Ainsi $S(\gamma^\epsilon) \le C_\epsilon(\gamma^\epsilon)$. De plus, comme $\gamma^\epsilon$ minimise $C_\epsilon$, $C_\epsilon(\gamma^\epsilon) \le C_\epsilon(\bar{\gamma}) = S(\bar{\gamma})$. Autrement dit, pour tout $\epsilon$ on a $S(\gamma^\epsilon) \le S(\bar{\gamma})$. Ainsi, par continuité de $S$, $S(\gamma^0) \le S(\bar{\gamma})$.

Montrons que $\gamma^0$ est un minimiseur de $K$. On a $C_\epsilon(\gamma^\epsilon) \le S(\bar{\gamma})$ par le raisonnement précédent. Ainsi~:
\[K(\gamma^\epsilon) \le K_{opt} + \epsilon (S(\bar{\gamma}) - S(\gamma^\epsilon)) \]
Le terme $S(\bar{\gamma}) - S(\gamma^\epsilon)$ est borné, donc par passage à la limite $K(\gamma^0) \le K_{opt}$. On a donc que $\gamma^0$ est un minimiseur de $K$, et qu'il a une entropie inférieure à celle de $\bar{\gamma}$. Par unicité du minimiseur de l'entropie sur le convexe $\argmin_{\Pi(\mu,\nu)} K$, on a $\gamma^0 = \bar{\gamma}$.

Ceci permet de conclure que l'on a bien la convergence de $\epsilon \mapsto \gamma^\epsilon$ vers $\bar{\gamma}$ en 0. En effet, dans le cas contraire, il existe un voisinage de $\bar{\gamma}$ et une suite de $\epsilon$ tendant vers 0, restant hors du voisinage. Or, on peut en extraire une suite qui converge vers $\gamma^0 = \bar{\gamma}$, ce qui est une contradiction.
\end{proof}

\subsection{Forme de la solution}
On pourrait croire que la régularisation entropique complique les calculs, car en forçant tous les coefficients à être non nuls, elle ajoute des degrés de liberté au problème. Pour deux histogrammes à $n$ points, on risque de se retrouver avec $n^2$ coefficients à calculer. En fait, nous allons voir que la solution du problème régularisé a une forme particulière, et que les inconnues sont seulement au nombre de $2n$, ceci grâce à des considérations de dualité.

\begin{definition}
Soit $L : A \times B \rightarrow \IR$. On appelle \emph{point-selle} de $L$ un couple $(a_0,b_0) \in A\times B$ tel que~:
\[ \forall (a,b) \in A\times B, \quad L(a_0,b) \le L(a_0,b_0) \le L(a,b_0) \]
\end{definition}

\begin{lemma}\label{lemma:pt_selle}
Le Lagrangien associé à $K^\epsilon$~:
\[
L(\gamma, p, q) = K^\epsilon(\gamma) +
 \sum_i p_i \left(\sum_j \gamma_{i,j} - \mu_i\right) + 
 \sum_j q_j\left(\sum_i \gamma_{i,j} - \nu_j\right) ,
\]
admet un point-selle $(\gamma^\epsilon,p^\epsilon,q^\epsilon)$, où $\gamma^\epsilon$ est, comme auparavent, le minimiseur de $K^\epsilon$.
\end{lemma}
\begin{proof}
On peut considérer que $K^\epsilon$ est défini sur tout $U = \IR_{+*}^{nm}$ par la même formule que sur $\Pi(\mu,\nu)$. Du fait que $\gamma^\epsilon$ est un minimum de $K^\epsilon$ sur $\Pi(\mu,\nu) \cap U$, on déduit que le gradient de $K^\epsilon$ en $\gamma^\epsilon$ est orthogonal à $\Pi(\mu,\nu)$. Autrement dit, il existe des multiplicateurs de Lagrange.

En effet, pour tout $v \in \IR^{nm}$ assez petit vérifiant $\sum_i v_{i,j} = \sum_j v_{i,j} = 0$, et $\eta \in \IR$, $|\eta|$ assez petit, on a~:
\[ \begin{split}
K^\epsilon(\gamma^\epsilon + \eta v) &\ge K^\epsilon(\gamma^\epsilon) \\
K^\epsilon(\gamma^\epsilon) + \eta \langle \nabla K^\epsilon(\gamma^\epsilon) , v \rangle + o(\eta)  &\ge K^\epsilon(\gamma^\epsilon) \\
K^\epsilon(\gamma^\epsilon) + \eta \langle \nabla K^\epsilon(\gamma^\epsilon) , v \rangle &\ge K^\epsilon(\gamma^\epsilon) \quad \text{pour $\eta$ assez petit} \\
\langle \nabla K^\epsilon(\gamma^\epsilon) , v \rangle &= 0 .
\end{split}\]
Le gradient de $K^\epsilon$ en $\gamma^\epsilon$ est donc dans l'orthogonal du noyau de $B : \gamma \mapsto ((\sum_j \gamma_{i,j})_i, (\sum_i \gamma_{i,j})_j)$, c'est-à-dire dans l'image de l'application linéaire transposée $B^\transpose : ((p_i)_i,(q_j)_j) \mapsto (p_i+q_j)_{i,j}$.

Il existe donc $(p^\epsilon,q^\epsilon)$ deux vecteurs tels que~:
\[\begin{split}
\nabla K^\epsilon(\gamma^\epsilon) &= B^\transpose (p^\epsilon,q^\epsilon)  \\
c_{i,j} + \epsilon (1+\log(\gamma_{i,j})) &= p^\epsilon_i + q^\epsilon_j .
\end{split}\]

Montrons qu'alors $(\gamma^\epsilon,p^\epsilon,q^\epsilon)$ est point-selle de $L$.

La première inégalité est ici une égalité, et découle du fait que pour tous $(p,q) \in \IR^{n+m}$, les termes impliquant $p$ et $q$ dans le Lagrangien sont nuls~:
\[ L(\gamma^\epsilon,p,q) = L(\gamma^\epsilon,p^\epsilon,q^\epsilon) = K^\epsilon(\gamma^\epsilon) .\]

La seconde inégalité de point-selle découle du fait que $\gamma^\epsilon$ minimise $K^\epsilon$, et que pour tout $\gamma \in \Pi(\mu,\nu)$ les autres termes du Lagrangien sont nuls.
\end{proof}


\begin{lemma}
Pour tout $\epsilon > 0$, le minimiseur $\gamma^\epsilon$ de $K^\epsilon$ est de la forme~:
\begin{equation}\label{eq:aGb}
\gamma^\epsilon = \diag(a)\;G\,\diag(b) ,
\end{equation}
où $a$ et $b$ sont deux vecteurs, uniques à un facteur près, et $G = (e^{-c_{i,j}/\epsilon})$ est l'exponentielle de $-c/\epsilon$, coefficient par coefficient.
\end{lemma}
\begin{proof}
Le Lagrangien admet un point-selle en $(\gamma^\epsilon,p^\epsilon,q^\epsilon)$, et il est dérivable en ce point (rappellons que $\gamma^\epsilon_{i,j}$ est à coefficients non nuls). Un point-selle étant notamment point critique, on sait que la différentielle du Lagrangien est nulle en ce point. Or la différentielle selon $\gamma_{i,j}$ s'écrit~:
\[\begin{split}
0 = \frac{\partial L}{\partial \gamma_{i,j}}
&= c_{i,j} + \epsilon(1+\log \gamma_{i,j} ) + p_i + q_j \\
\log \gamma_{i,j} &= \frac{-c_{i,j} - p_i - q_j}{\epsilon} -1\\
\gamma_{i,j} &= e^{-1/2 - p_i / \epsilon} e^{-c_{i,j}/\epsilon} e^{-1/2 - q_j / \epsilon} .
\end{split}\]
On en déduit l'existence de $a$ et $b$. L'unicité à une constante près vient du fait que l'on doit avoir \[\begin{split}
\forall i,j,\quad
a_i - c_{i,j}/\epsilon + b_j &= -1/2 - p_i/\epsilon - c_{i,j} / \epsilon - 1/2 - q_j/ \epsilon \\
a_i + p_i/\epsilon &= -1 - b_j - q_j/\epsilon .
 \end{split}\]
Chaque membre de l'égalité ne dépendant que d'une variable à la fois, on en déduit que la quantité est indépendante de $i$ et $j$~: $a + p/\epsilon = -1-b-q/\epsilon = t \in \IR$. Ce paramètre $t$ détermine alors $a$ et $b$ de façon unique.
\end{proof}

La difficulté du problème initial résidait dans le fait de trouver quelles contraintes sont actives (i.e. quels coefficients de $\gamma$ sont nuls). Pour le problème régularisé, aucune contrainte n'est active. Ceci semblait impliquer que de nombreux calculs étaient nécessaires pour déterminer les $n^2$ coefficients de $\gamma$. Seulement, comme nous venons de le montrer, la solution du problème a une forme particulière $\gamma^\epsilon = \diag(a)\;G\,\diag(b)$. Le calcul du plan optimal consistera donc à trouver les $2n$ coefficients de $a$ et $b$.

Remarquons que, bien que le calcul de $W^\epsilon(\mu,\nu) = K^\epsilon(\gamma^\epsilon)$ nous fournisse encore une mesure de la disparité de deux mesures en termes spatiaux, il ne s'agit plus d'une \emph{distance} au sens mathématique. En effet, la régularisation entropique empêche $\gamma^\epsilon$ d'avoir son support inclus dans la diagonale dans le cas où $\mu = \nu$ si l'espace est non trivial. On n'aura donc pas $W^\epsilon (\mu,\mu) = 0$ en général.

\section{Algorithme de Sinkhorn}

\subsection{Algorithme}
L'algorithme permettant de calculer $a$ et $b$ est le suivant \cite{cuturi13}~:

\begin{table}[h!]
\centering
\caption*{Algorithme de Sinkhorn}
\label{algorithme}
\begin{tabular}{|l|}
\hline
\'Etant donnés ($(c_{i,j})$, $\epsilon$, $(\mu_i)_{1\le i \le n}$,$(\nu_j)_{1 \le j \le m}$), on pose~: \\
$a^0 = (1/n, \ldots, 1/n)$;\\
$(G_{i,j}) = (e^{c_{i,j}/\epsilon})$;\\
Puis on itère (jusqu'à ce qu'un critère d'arrêt convenable soit vérifié)~: \\
$\quad b^{k+1}_j = \nu_j / (G^\transpose a^k)_j$;\\
$\quad a^{k+1}_i = \mu_i /(G b^{k+1})_i$; \\
\hline
\end{tabular}
\end{table}
Intuitivement, l'algorithme impose les contraintes marginales alternativement sur les lignes et les colonnes. Il le fait de façon à effectuer un certain type de projection, comme montré par la suite dans la preuve de convergence.

\subsection{Convergence}
\begin{theoreme}[Convergence et correction de l'algorithme]
\label{th:sink_cv}
Si on note $\gamma^k = \diag(a^k)\;G\,\diag(b^k)$ les résultats des itérations de l'algorithme précédent, la suite $(\gamma^k)$ converge et~:
\[\lim_{k \rightarrow +\infty} \gamma^k = \gamma^\epsilon .\]
\end{theoreme}

Afin de prouver ce résultat, on utilise un raisonnement préliminaire.
\begin{definition}[Divergence de Kullback-Leibler]
On définit la \emph{divergence de Kullback-Leibler} par la formule~:
\begin{equation}\label{eq:kl}
KL(\gamma | \xi) = \sum_{i,j} \gamma_{i,j} \left(\log\left(\frac{\gamma_{i,j}}{\xi_{i,j}} \right) -1 \right) .
\end{equation}
\end{definition}
On peut considérer la divergence comme une façon de mesurer la disparité de deux répartitions.
\begin{definition}[Projection de Kullback-Leibler]
 \'Etant donnés un convexe fermé $\Ccal$ et une répartition $\xi$, on définit la \emph{projection de Kullback-Leibler} de $\xi$ sur $\Ccal$ par~:
\begin{equation}\label{eq:pkl}
P^{KL}_{\Ccal} (\xi) = \argmin_{\gamma \in \Ccal} KL(\gamma | \xi) .
\end{equation}
Ceci définit bien une unique valeur car $KL(\cdot | \xi)$ est une fonctionnelle strictement convexe.
\end{definition}


\begin{lemma}\label{lemma:bregman}
Soit $\Ccal_1, \ldots, \Ccal_l$ une famille de sous-espaces affines de l'espace $\IR_{+*}^{nm}$, et $\Ccal = \bigcap_i \Ccal_i$ leur intersection. On prend $\xi_0 \in \IR_{+*}^{nm}$ quelconque, et on construit la suite $(\xi_i)$ par récurrence de la façon suivante (où les indices sont à comprendre modulo $l$)~:
\[
\xi_{i+1} = P^{KL}_{\Ccal_{i+1}}(\xi_i) .
\]
Alors la suite $(\xi_i)$ converge vers $P^{KL}_{\Ccal} (\xi_0)$. 
\end{lemma}
Ce résultat est démontré dans \cite{bregman66}.

Un dernier résultat intermédiaire est nécessaire afin de pouvoir conclure. On observe qu'à l'aide de la divergence de Kullback-Leibler, le problème de minimisation (\ref{eq:keps}) se réécrit~:
\[
\gamma^\epsilon = P^{KL}_{\Pi(\mu,\nu)} (G)
\quad \text{où} \quad G = e^{-c/\epsilon} .
\]
En effet, on a l'identité~:
\[
\forall \gamma \in \Pi(\mu,\nu), 
\quad K^\epsilon(\gamma) 
= \sum_{i,j} \gamma_{i,j} (c_{i,j} + \epsilon \log(\gamma_{i,j}))
= \frac{KL(\gamma | G)}{\epsilon} .
\]
Notons $\Ccal_1$ et $\Ccal_2$ les ensembles~:
\[
\Ccal_1 = \left\{ \gamma \in \IR_{+*}^{nm} \;\middle|\; \sum_j \gamma_{i,j} = \mu_i \right\}
; \quad
\Ccal_2 = \left\{ \gamma \in \IR_{+*}^{nm} \;\middle|\; \sum_i \gamma_{i,j} = \nu_j \right\} .
\]
On a notamment $\Pi(\mu,\nu) = \Ccal_1 \cap \Ccal_2$.
\begin{lemma}\label{lemma:pkl_c1c2}
Pour $\gamma \in \IR_{+*}^{nm}$, on a~:
\begin{equation}
P^{KL}_{\Ccal_1} (\gamma) = \diag \left(\frac{\mu}{\gamma \One} \right) \gamma
\quad et \quad
P^{KL}_{\Ccal_2} (\gamma) = \gamma \diag \left(\frac{\nu}{\gamma^\transpose \One} \right)
.
\end{equation}
Autrement dit, la projection normalise les lignes (respectivement les colonnes) de $\gamma$ afin que le résultat ait la marginale souhaitée.
\end{lemma}
\begin{proof}
Prenons $\gamma \in \IR^{nm}$, et posons $F(\zeta) = KL(\zeta | \gamma)$. Pour $\zeta^* = P^{KL}_{\Ccal_1} (\gamma)$, du fait que $\zeta^*$ minimise $F$ sur $\Ccal_1$ on doit avoir $\nabla F (\zeta^*) \in (\Ccal_1)^\transpose$ (par un raisonnement similaire au lemme \ref{lemma:pt_selle}). Ceci signifie que $(\nabla F (\zeta^*))_{i,j}$ ne dépend pas de $j$. On appelle cette quantité $\lambda_i$. Or le calcul donne~:
\[
\left(\nabla F (\zeta^*)\right) _{i,j} = \log \left(\frac{\zeta^*_{i,j}}{\gamma_{i,j}} \right) = \lambda_i .
\]
On en déduit que $\zeta^*_{i,j} = \gamma_{i,j} e^{\lambda_i}$. Enfin, comme il faut que $\sum_{j} \zeta^*_{i,j} = \mu_i$, on trouve~:
\[\begin{split}
e^{\lambda_i} \sum_j \gamma_{i,j} &= \mu_i \\
e^{\lambda_i} &= \frac{\mu_i}{\sum_{j} \gamma_{i,j}} \\
\zeta_{i,j} &= \gamma_{i,j} \frac{\mu_i}{\sum_{j'} \gamma_{i,j'}} .
\end{split}\]
D'où le résultat.
\end{proof}
On peut maintenant démontrer la convergence et la correction de l'algorithme.
\begin{proof}[Démonstration du théorème \ref{th:sink_cv}]
Le minimiseur $\gamma^\epsilon$ est le projeté de $G$ sur $\Ccal_1 \cap \Ccal_2$ selon la divergence de Kullback-Leibler. L'algorithme consiste à construire la suite des projections alternativement sur $\Ccal_1$ et $\Ccal_2$, comme dans le lemme \ref{lemma:bregman}~:
\[\gamma^{2k} = a^k G b^k,\quad \gamma^{2k+1} = a^k G b^{k+1} .\]
D'après le lemme \ref{lemma:pkl_c1c2}, il s'agit bien des projections sur les sous-espaces affines $\Ccal_1$ et $\Ccal_2$.
 Ainsi, la suite des itérés converge vers $P^{KL}_{\Pi(\mu,\nu)}(G) = \gamma^\epsilon$. Ceci prouve la convergence de l'algorithme, et que la limite est correcte.
\end{proof}

\subsection{Calcul de barycentres}
Un problème plus général que l'on peut se poser est celui de l'interpolation de mesures. On aimerait, étant données $(\nu^1, \ldots, \nu^s)$ famille de mesures, et $(\lambda_1, \ldots, \lambda_s)$ famille de coefficients de pondération positifs de somme 1, trouver un \og barycentre \fg $\,\mu$ interpolant les $\nu^i$ de façon convenable relativement à la métrique de Wasserstein \cite{agueh11}.

\begin{definition}[Barycentre de transport optimal]
Soit $\epsilon > 0$. Soient $\nu^1, \ldots, \nu^s \in (\IR^{n})^s$ et $\lambda_1, \ldots, \lambda_s \in (\IR_+)^s$ tels que $\sum_k \lambda_k = 1$. On définit le barycentre de transport optimal de $(\nu^k)$ pondéré par $(\lambda_k)$ par~:
\[
\mu = \argmin_{\IR_{+}^{n}} \min_{(\gamma_1 \ldots \gamma_s) \in \Pi_\mu} \sum_k \lambda_k K^\epsilon(\gamma^k) 
\quad \text{où} \quad
\Pi_\mu = \Pi(\mu,\nu_1) \times \ldots \times \Pi(\mu,\nu_s) .
\]
\end{definition}
Autrement dit, $\mu$ est la répartition qui minimise la moyenne pondérée des coûts de transport optimal vers $\nu$~:
\[ \mu = \argmin \sum_k \lambda_k W^\epsilon_p(\mu,\nu^k) . \]

Dans le cas du transport régularisé, une variante de l'algorithme de Sinkhorn permet de calculer le barycentre des $(\nu^k)$ \cite{benamou15}~:

\begin{table}[h!]
\centering
\caption*{Algorithme de Sinkhorn pour le calcul de barycentre}
\label{algorithme}
\begin{tabular}{|l|}
\hline
\'Etant donnés $(c_{i,j})$, $\epsilon$, $(\nu^1,\ldots,\nu^s)$,$(\lambda_1, \ldots, \lambda_s)$, on pose~: \\
$b^0 = ones(n,s)$;\\
$(G_{i,j}) = (e^{c_{i,j}/\epsilon})$;\\
Puis on itère (jusqu'à ce qu'un critère d'arrêt convenable soit vérifié)~: \\
\tab$\forall j, \quad a^{k+1}_j = \nu^j /(G b^k_j)$;\\
\tab\tab$\quad\mu^{k+1} = \exp\left(\sum_j \lambda_j \log(b^k_j \odot G a^{k+1}_j) \right)$;\\
\tab$\forall j, \quad b^{k+1}_j = \mu^{k+1} /(G a^{k+1}_j)$; \\
\hline
\end{tabular}
\end{table}
La notation $\odot$ désigne le produit élément par élément. Cet algorithme alterne entre la normalisation des marginales vers l'estimation actuelle de $\mu$, et l'actualisation de l'estimation de $\mu$.

Dans \cite{benamou15}, il est montré que cet algorithme est correct et que la suite $(\mu^k)$ converge vers le barycentre du problème régularisé. Tout comme l'algorithme de Sinkhorn, cet algorithme a une structure très simple, uniquement basée sur des calculs matriciels. Ceci permet de les effectuer sur un GPU, processeur optimisé pour ce type de tâche, et de bénéficier d'une importante accélération matérielle.

\section{Exemples et applications}

\subsection{Sinkhorn unidimensionnel}
Un premier exemple sur lequel on voit apparaître comment se comporte la régularisation entropique est celui de mesures à une dimension. On veut par exemple connaître le plan optimal pour des sommes de deux répartitions gaussiennes, pour un coût quadratique en la distance euclidienne.

Le premier élément que l'on observe est la convergence de l'algorithme. Un résultat supplémentaire sur cet algorithme est qu'il converge \emph{linéairement} \cite{benamou15}. L'erreur $|| (\gamma^k)^\transpose \One - \nu ||$ tend donc vers zéro de manière géométrique (jusqu'à ce qu'on arrive aux limites de précision numérique). De plus cette convergence est de plus en plus lente si on diminue $\epsilon$, comme illustré figure \ref{fig:1d_cv}.

Les plans de transport obtenus (figure \ref{fig:1d_maps}) montrent les effets attendus de la régularisation entropique~: on obtient des objets plus lisses, et d'autant plus lisses que $\epsilon$ est grand. Pour $\epsilon$ très grand, le plan de transport tend vers l'extrémum de l'entropie sur $\Pi(\mu,\nu)$, c'est-à-dire le produit tensoriel $\gamma_{i,j} = \mu_i \nu_j$. C'est effectivement ce que l'on observe pour $\epsilon=10$. À l'opposé, plus $\epsilon$ est petit, plus la mesure a tendance à devenir concentrée.

\begin{figure}
\centering
\includegraphics[width=10cm]{1d_err.png}
\caption{Diminution de l'erreur au fil des itérations de Sinkhorn, pour différentes valeurs de $\epsilon$}
\label{fig:1d_cv}
\end{figure}

\begin{figure}
\centering
\begin{subfigure}[b]{5cm}
\includegraphics[width=5cm]{1d_interp_0_1.png}
\caption{$\epsilon=10$}
\end{subfigure}
%
\begin{subfigure}[b]{5cm}
\includegraphics[width=5cm]{1d_interp_0_5.png}
\caption{$\epsilon=2$}
\end{subfigure}
%
\begin{subfigure}[b]{5cm}
\includegraphics[width=5cm]{1d_interp_2.png}
\caption{$\epsilon=0.5$}
\end{subfigure}
%
\begin{subfigure}[b]{5cm}
\includegraphics[width=5cm]{1d_interp_18.png}
\caption{$\epsilon=0.05$}
\end{subfigure}
%
\caption{Plans de transports pour différentes valeurs de $\epsilon$}
\label{fig:1d_maps}
\end{figure}

\subsection{Applications du calcul de barycentre}

Parmi les nombreuses applications du transport optimal, celles que nous avons choisi de mettre en \oe uvre dans le cadre de ce mémoire sont liées au calcul de barycentre. Il est question d'interpolation de formes, où on s'intéresse au fait de trouver des barycentres entre des parties de $[0;1]^2$ ; et d'interpolation de nuages de mots, où on se sert d'un \emph{word embedding} \cite{mikolov13} pour trouver des histogrammes intermédiaires prenant en compte la sémantique.

\subsubsection{Interpolation de formes}
Une utilisation assez basique du transport optimal est l'application à l'interpolation de formes géométriques. 
Afin de se placer dans le cadre numérique adapté, on identifie des images bidimensionnelles à des répartitions discrètes.
On utilise ensuite l'algorithme de Sinkhorn pour calculer les barycentres de ces répartitions.
Le résultat est illustré figure \ref{fig:2d_interp}.
Un fichier de démonstration \texttt{barycentre\_demo.py} est inclus dans le dossier.

\begin{figure}
\centering
\includegraphics[height=10cm]{2d_interp.png}
\caption{Interpolation de formes géométriques bidimensionnelles par transport optimal}
\label{fig:2d_interp}
\end{figure}

\subsubsection{Interpolations de nuages de mots}
Le transport optimal régularisé, calculé à l'aide de l'algorithme de Sinkhorn, fonctionne de la même façon quelle que soit la dimension de l'espace considéré. On peut donc effectuer le calcul dans un espace de grande dimension. Une utilisation possible est l'estimation de la distance entre deux textes, en termes de champ lexical.

Dans ce but, nous avons utilisé un \emph{word embedding}, ou plongement lexical \cite{mikolov13}. Un word embedding $p$ est une application qui, à chaque mot d'une langue, associe un vecteur dans un espace de dimension $N$ (où $N$ est généralement de l'ordre de quelques centaines). Les contraintes sur ce plongement le rendent utile pour des questions d'ordre sémantique~:
\begin{itemize}
\item Les points voisins ont des sens proches. Par exemple, les différents noms reliés à une même espèce animale seront peu éloignés.
\item Les vecteurs $p(b_i) - p(a_i)$ sont similaires pour des familles de mots où $a_i$ et $b_i$ ont la même relation sémantique ; par exemple \og homme \fg et \og femme \fg, \og roi \fg et \og reine \fg, ou \og oncle \fg et \og tante \fg. De la même façon, en anglais les superlatifs \og stronger \fg, \og softer \fg, etc. seront reliés à l'adjectif correspondant par des vecteurs similaires.
\end{itemize}
De tels plongements sont utilisés dans diverses applications d'intelligence artificielle qui travaillent avec du texte. Il en existe plusieurs : nous avons ici utilisé \og GloVe \fg \cite{glove}, sur un dictionnaire anglais.

Ici, le plongement sert à mettre une distance sur l'ensemble des mots. Le coût de transport d'un mot vers l'autre est le carré de la distance euclidienne entre les vecteurs correspondants. L'ensemble des mots devient alors un espace métrique discret, sur lequel on peut donc calculer un transport et trouver des barycentres de la même façon que précédemment.

\begin{figure}
\centering
\includegraphics[width=14cm]{text_romeo_obama.png}
\caption{Interpolation de nuages de mots entre \emph{Roméo et Juliette} et l'article Wikipédia ``Barack Obama''.}
\label{fig:romeo_obama}
\end{figure}

Afin de représenter graphiquement un histogramme de mots, la librairie \texttt{wordcloud} de Python a été utilisée. Elle permet de facilement créer des nuages de mots, où les mots les plus communs apparaissent plus gros.
Un exemple de résultat est proposé figure \ref{fig:romeo_obama}.
On observe que les mots qui apparaissent souvent sont en fait ceux des champs lexicaux proches que les deux textes évoquent : \emph{time, good, care}.
On peut s'amuser du fait que les mots \emph{act} et \emph{house} apparaissent assez gros dans le nuage du milieu, alors qu'ils ont des sens bien différents entre les deux textes.
Mais on remarque surtout que dans les histogrammes médians, un nombre limité de mots cumule la plupart du poids, et qu'il s'agit de mots ayant des sens et des utilisations très diverses (tels que \emph{time}, \emph{good}, \emph{tell} ou \emph{make}).
Une interprétation possible est que ces mots sont relativement centraux dans le plongement lexical, et que pour passer d'un mot à un autre sans aucun lien, il est préférable de passer par des mots \og neutres \fg.

Ce traitement permet d'estimer la similarité lexicale des textes, et donc de les regrouper selon les champs lexicaux utilisés. On remarque que l'algorithme regroupe très bien les textes d'un même auteur~: les différents textes d'Aristote, de Jefferson, les pièces de Shakespeare, entre autres, sont bien regroupés entre eux. L'algorithme permet aussi de déterminer quels livres utilisent des champs lexicaux communs~: ceux-là se retrouvent assez proches de nombreux autres ; c'est le cas de \emph{Harry Potter}, qui est rapproché des \oe uvres de Woolf et de Sevenson.

Le transport optimal entre nuages de mots permet donc de facilement faire ressortir certains aspects d'un texte contenus dans son champ lexical.
Il permet également de s'amuser à imaginer à quoi ressemblerait un livre écrit par un auteur à mi-chemin entre --- par exemple --- Dickens et Nietzsche.


\section{Conclusion}

Le transport optimal fournit une distance naturelle entre des mesures, appelée \emph{métrique de Wasserstein}. À la différence des normes $l^p$, cette distance prend en compte la structure métrique de l'espace sous-jacent. Elle permet d'accomplir différentes tâches, par exemple de classification de différents types d'histogrammes. On en tire aussi une notion de barycentre, qui permet des interpolations d'objets variés.

Le transport optimal \emph{régularisé} est un problème strictement convexe approximant de manière naturelle le transport optimal. La forme particulière du problème permet de le résoudre assez simplement par l'\emph{algorithme de Sinkhorn}, qui consiste en de simples opérations matricielles.
Une variante de l'algorithme de Sinkhorn permet de également de calculer des barycentres.
La complexité de calcul est bien inférieure à celle des méthodes de programmation linéaire qui résolvent le problème initial, et les solutions sont de bonnes approximations, quoique plus lisses.
C'est pourquoi le transport optimal régularisé est un outil puissant pour de nombreuses applications telles que le traitement d'images ou le \emph{machine learning}.

\medskip
\begin{thebibliography}{99}
\bibitem{villani08}
Villani, Cédric. \textit{Optimal transport: old and new.} Vol. 338. Springer Science \& Business Media, 2008.
\bibitem{santambrogio15}
Santambrogio, Filipo. \textit{Optimal Transport for Applied Mathematicians}. Progress in Nonlinear Differential Equations and Their Applications, vol. 87. Birkhäuser, 2015.
\bibitem{cuturi13}
Cuturi, Marco. ``Sinkhorn distances: Lightspeed computation of optimal transport.'' \textit{Advances in Neural Information Processing Systems.} 2013.
\bibitem{bregman66}
L. M. Bregman. ``The relaxation method of finding the common point of
convex sets and its application to the solution of problems in convex programming.''
\textit{USSR computational mathematics and mathematical physics},
7(3):200–217, 1967.
\bibitem{benamou15}
Benamou, Jean-David, et al. ``Iterative Bregman projections for regularized transportation problems.'' \textit{SIAM Journal on Scientific Computing} 37.2 (2015): A1111-A1138.
\bibitem{pele09}
Pele, Ofir and Werman, Michael (2009). \textit{Fast and robust earth mover’s distances}. In ICCV’09.
\bibitem{kantorovich42}
L. Kantorovich. ``On the transfer of masses'' (russe). \textit{Doklady Akademii
Nauk}, 37(2):227–229, 1942.
\bibitem{agueh11}
Agueh, Martial, and Carlier, Guillaume. ``Barycenters in the Wasserstein space.'' \textit{SIAM Journal on Mathematical Analysis} 43.2 (2011): 904-924.
\bibitem{mikolov13}
Mikolov, Thomas et al. ``Distributed representations of words and phrases and their compositionality.'' \textit{Advances in neural information processing systems}. 2013.
\bibitem{glove}
Pennington, Jeffrey, Richard Socher and Christopher D. Manning. ``Glove: Global Vectors for Word Representation.'' \textit{EMNLP}. Vol. 14. 2014.
\end{thebibliography}

\end{document}